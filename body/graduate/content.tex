% =============================================================================
% 第一章 绪论
% =============================================================================
\chapter{绪论}
\label{chap:intro}
% 1.1 研究背景与意义
\section{研究背景与动机}

近年来，随着实时渲染技术与游戏引擎的飞速发展，虚拟制作（Virtual Production）已成为影视工业、动画创作和数字内容生产领域的重要技术范式\cite{kadner2019virtual}。以Unreal Engine 5为代表的现代游戏引擎，凭借Nanite微多边形几何技术和Lumen全局光照系统\cite{karis2021nanite,kato2022lumen}，实现了电影级画质的实时渲染，使得创作者能够在三维虚拟环境中即时预览最终视觉效果。这一技术革新极大地缩短了传统影视制作中“创意构思—渲染等待—效果确认”的漫长迭代周期，为虚拟摄影棚、实时动画预演和交互式叙事等应用场景奠定了坚实的技术基础\cite{goualard2022taxonomy}。

在虚拟制作的完整工作流程中，虚拟摄像机扮演着连接创意表达与技术实现的关键角色。正如传统电影摄影中摄影师通过镜头语言传递叙事情感，虚拟环境中的摄像机同样承载着构图、景别、角度和运动等丰富的视觉表意功能\cite{arijon1976grammar,mascelli1965five}。一个恰当的机位选择能够引导观众视线、营造情绪氛围、强化叙事张力；而一段流畅的运镜设计则可以建立空间关系、创造视觉节奏、实现场景过渡。因此，如何高效、精准地控制虚拟摄像机，成为影响虚拟制作质量与效率的核心问题之一。

然而，当前虚拟摄像机的控制方式仍然面临显著的交互瓶颈。在主流的三维创作软件和游戏引擎中，摄像机的配置通常需要用户手动调整一系列数值参数，包括三维空间中的位置坐标、欧拉角或四元数表示的旋转姿态、视场角（Field of View）等。这种参数化的交互方式虽然提供了精确的控制能力，却在实际使用中暴露出诸多问题。首先，参数空间的高维性与抽象性使得非技术背景的创作者难以建立直观的操作认知——当美术设计师希望获得一个“仰拍主体的近景镜头”时，其脑海中浮现的是具体的视觉画面，而非位置向量$(x, y, z)$或旋转角度$(\alpha, \beta, \gamma)$的数值组合。其次，专业的电影摄影术语体系与引擎的参数表示之间缺乏天然的对应关系，“中景”“过肩镜头”“推轨”等约定俗成的镜头语言无法直接转化为可执行的参数指令。

这种认知鸿沟在跨专业协作的生产环境中表现得尤为突出。在实际的虚拟制作项目中，导演或美术指导往往使用电影摄影的专业术语描述其镜头设计意图，例如“给一个低角度仰拍火箭升空的大全景，摄像机缓慢推进”。然而，负责技术实现的工程师需要将这种自然语言描述逐一转换为引擎可识别的数值参数：首先估算摄像机与目标物体的合理距离以满足“大全景”的景别要求，然后计算实现“低角度仰拍”效果所需的俯仰角，接着规划“缓慢推进”运动的起止位置与时间曲线，最后在引擎中反复调试各项参数直至视觉效果符合创意预期。这一过程往往需要美术人员与技术人员之间进行多轮沟通与迭代，不仅消耗大量时间成本，也容易因理解偏差而导致返工。创意支持工具的相关研究表明\cite{shneiderman2007creativity,frich2019creativity}，降低工具的使用门槛、缩短创意到实现的路径，对于提升创作效率和激发创新潜能具有重要意义。

针对虚拟摄像机的自动控制问题，学术界已开展了数十年的持续研究。早期工作主要基于约束满足的思想，将摄像机配置问题建模为满足一组视觉约束（如目标可见性、屏幕位置、相对角度等）的求解问题\cite{gleicher1992through,bares1999constraint}。后续研究进一步引入优化方法和搜索策略，探索了势场法\cite{li2008real}、可见性图\cite{oskam2009visibility}、环面空间（Toric Space）\cite{lino2015intuitive}等多种技术路线。近年来，随着深度学习的兴起，基于强化学习和模仿学习的方法也被应用于无人机航拍等场景\cite{bonatti2020autonomous,nageli2017real}。这些方法在各自的问题设定下取得了良好效果，但普遍存在一个共同局限：它们要求用户以形式化的方式指定约束条件或目标函数，而非直接接受自然语言形式的镜头描述。换言之，“如何理解用户的镜头意图”这一前端问题，在既往研究中尚未得到充分关注。

大语言模型（Large Language Model, LLM）的突破性进展为上述问题的解决带来了新的契机。以GPT-4\cite{openai2023gpt4}、LLaMA\cite{touvron2023llama2}为代表的大语言模型展现出强大的自然语言理解能力、常识推理能力和指令遵循能力\cite{ouyang2022training,wei2022chain}。更为重要的是，通过精心设计的提示工程（Prompt Engineering）\cite{sahoo2024systematic}，大语言模型能够将非结构化的自然语言输入转换为结构化的输出格式，如JSON、代码或领域特定语言，这为构建“自然语言—形式化表示”的转换桥梁提供了可能。在具身智能（Embodied AI）领域，研究者已成功探索了利用大语言模型实现语言到机器人控制的端到端映射\cite{liang2023code,driess2023palme,huang2023voxposer}，证明了大语言模型在理解空间关系、生成执行指令方面的潜力。与此同时，文本到视频生成领域的最新进展也开始关注摄像机运动的可控性问题\cite{wang2024motionctrl,he2024cameractrl}，但这些工作主要面向生成式模型的条件控制，而非实时引擎中的精确参数计算。

综上所述，虚拟制作技术的快速发展催生了对智能化摄像机控制工具的迫切需求，而大语言模型的能力跃迁则为实现“自然语言驱动的虚拟摄像机控制”这一目标提供了技术可行性。本文旨在探索一种新的研究范式：以自然语言作为人机交互的媒介，借助大语言模型的语义理解能力解析用户的镜头意图，并结合计算机图形学的几何算法生成精确的摄像机参数，最终在虚拟引擎中实现自动化的镜头呈现。这一研究不仅有望降低虚拟摄像机的操作门槛、提升跨专业协作效率，也将为人工智能赋能数字内容创作提供新的思路与实践参考。

\section{国内外研究现状}

本文的研究涉及虚拟摄像机自动控制、大语言模型的结构化输出与推理、以及自然语言驱动的三维场景交互等多个领域。本节将从这三个维度对国内外相关研究进行梳理与分析。

\subsection{虚拟摄像机自动控制}

虚拟摄像机的自动控制问题自计算机图形学发展早期便受到研究者的持续关注。Christie等人\cite{christie2008camera}在其综述中系统总结了虚拟环境中摄像机控制的主要方法与技术演进脉络。早期的代表性工作可追溯至Gleicher和Witkin\cite{gleicher1992through}提出的“透镜式”（Through-the-Lens）交互范式，该方法允许用户通过直接操纵画面中的投影结果来间接控制摄像机参数，开创了以视觉反馈驱动摄像机配置的研究思路。Bares等人\cite{bares1999constraint}则将摄像机配置建模为约束满足问题（Constraint Satisfaction Problem），通过定义目标可见性、屏幕位置、视角方向等约束条件，利用搜索算法求解满足全部约束的摄像机状态。这一形式化框架为后续研究奠定了重要基础。

在约束满足范式的基础上，研究者们探索了多种优化策略以提升求解效率和结果质量。Li和Chen\cite{li2008real}引入人工势场法，将摄像机的运动规划类比于机器人路径规划问题，通过构建吸引势场和排斥势场实现目标跟踪与障碍规避的平衡。Oskam等人\cite{oskam2009visibility}提出可见性转换图方法，预先计算场景中不同视点之间的可见性关系，从而支持在动态环境中进行高效的摄像机路径规划。Lino和Christie\cite{lino2015intuitive}提出的环面空间（Toric Space）理论是该领域的重要里程碑，他们将双目标构图问题映射到环面流形上进行参数化表示，使得复杂的摄像机配置问题转化为低维空间中的优化问题，显著提升了交互式摄像机控制的直观性与效率。

随着深度学习技术的发展，基于学习的摄像机控制方法逐渐兴起。在无人机摄影领域，Bonatti等人\cite{bonatti2020autonomous}结合艺术构图原则与强化学习算法，实现了自主无人机的智能航拍轨迹规划，能够在保证安全性和避障的前提下生成符合电影美学的拍摄路径。Nägeli等人\cite{nageli2017real}针对多无人机协同拍摄场景，提出了实时轨迹优化算法，支持动态目标的多视角覆盖。Galvane等人\cite{galvane2018directing}则专注于无人机电影拍摄的导演式控制，将高层镜头语义（如“跟随”“环绕”）转换为无人机的飞行轨迹。在虚拟环境中，Xie等人\cite{xie2023gait}利用深度强化学习生成室内场景的美学漫游路径，展示了学习方法在复杂三维环境中的适用性。尽管这些方法取得了显著进展，但它们通常依赖于特定格式的输入接口（如预定义的镜头类型标签或轨迹参数），尚未实现对自由形式自然语言描述的直接理解与处理。

\subsection{大语言模型与结构化输出}

大语言模型近年来取得了突破性进展，为自然语言驱动的智能系统构建提供了新的技术基础。自Vaswani等人\cite{vaswani2017attention}提出Transformer架构以来，基于自注意力机制的语言模型在规模和能力上持续提升。GPT-4\cite{openai2023gpt4}、LLaMA\cite{touvron2023llama,touvron2023llama2}等大语言模型展现出强大的自然语言理解、常识推理和指令遵循能力。Ouyang等人\cite{ouyang2022training}提出的指令微调（Instruction Tuning）技术通过人类反馈强化学习显著提升了模型对用户意图的理解与响应质量。Wei等人\cite{wei2022chain}发现的思维链（Chain-of-Thought）提示策略则揭示了大语言模型在复杂推理任务上的潜力。

在大语言模型的应用层面，结构化输出能力是实现语言到可执行指令转换的关键。Sahoo等人\cite{sahoo2024systematic}在其综述中系统总结了提示工程的各类技术与应用场景，指出通过精心设计的提示模板和输出格式约束，大语言模型能够稳定地生成JSON、XML、代码等结构化内容。这一能力为构建“自然语言—形式化表示—系统执行”的处理管线提供了可能。在视觉语言领域，CLIP\cite{radford2021learning}通过对比学习建立了图像与文本之间的语义关联，BLIP\cite{li2022blip}和LLaVA\cite{liu2023visual}等多模态模型进一步实现了图像理解与语言生成的统一，为视觉内容的语义化描述提供了技术支撑。

\subsection{自然语言驱动的三维场景交互}

将大语言模型应用于三维场景理解与控制是当前人工智能研究的前沿方向之一。在具身智能领域，Liang等人\cite{liang2023code}提出“代码即策略”（Code as Policies）范式，利用大语言模型直接生成机器人控制代码，实现了从语言指令到物理执行的端到端映射。Driess等人\cite{driess2023palme}提出的PaLM-E模型将语言模型与视觉感知深度融合，展示了具身多模态推理的可行性。Huang等人\cite{huang2023voxposer}提出VoxPoser方法，通过大语言模型生成三维价值图来引导机器人操作，实现了对自然语言任务描述的空间化理解。Rana等人\cite{rana2023sayplan}的SayPlan系统则利用三维场景图作为中间表示，支持大语言模型在复杂环境中进行长程任务规划。这些工作表明，大语言模型具备理解空间关系、生成三维控制指令的能力，为将其应用于虚拟摄像机控制提供了重要参考。

在视频生成领域，摄像机运动的可控性近年来受到广泛关注。Wang等人\cite{wang2024motionctrl}提出MotionCtrl方法，通过解耦摄像机运动与物体运动，实现了对视频生成过程中摄像机轨迹的独立控制。He等人\cite{he2024cameractrl}的CameraCtrl工作进一步探索了基于Plücker坐标的摄像机姿态表示，提升了文本到视频生成中摄像机控制的精度。Yang等人\cite{yang2024directavideo}提出Direct-a-Video方法，支持用户通过简单的运动指示词（如“推进”“环绕”）指定摄像机运动类型。Jiang等人\cite{jiang2024exceptional}则专注于文本到摄像机轨迹的直接生成，提出了具有角色感知能力的轨迹生成方法。然而，这些工作主要服务于生成式视频模型的条件控制，其输出为隐空间的运动表示或像素级视频，而非可在实时引擎中直接执行的精确摄像机参数。

\subsection{现有研究的不足}

综合以上分析可以发现，现有研究在以下几个方面仍存在不足：第一，传统的虚拟摄像机控制方法虽然提供了成熟的数学建模框架和优化算法，但其输入接口依赖于形式化的约束定义或预设的镜头类型，缺乏对自然语言的直接支持；第二，大语言模型在具身智能等领域的成功应用证明了其理解空间关系和生成控制指令的能力，但针对虚拟摄像机控制这一特定任务的研究尚属空白；第三，视频生成领域的摄像机控制工作关注的是生成模型的条件输入，与实时引擎中基于精确几何计算的摄像机控制存在本质差异。因此，如何将大语言模型的语义理解能力与传统摄像机控制的几何计算方法相结合，构建自然语言驱动的虚拟摄像机控制系统，是一个具有理论价值和应用前景的研究问题。

\section{研究内容与主要贡献}

\subsection{研究目标}

本文旨在研究一种基于大语言模型的虚拟摄像机智能控制方法，实现从自然语言描述到摄像机参数的自动转换。具体而言，系统接受两类输入：用户以自然语言形式表达的镜头描述（如“给火箭一个低角度仰拍的近景”）以及从虚拟场景中获取的目标物体信息（包括位置、尺寸、朝向等几何属性）；系统的输出分为两种形式：对于静态镜头需求，输出摄像机的初始配置参数（位置、旋转、视场角）；对于动态运镜需求，输出摄像机的运动轨迹序列。整个系统在Unreal Engine 5.3.2平台上实现，支持实时预览与交互式调整。

\subsection{研究内容}

围绕上述研究目标，本文的研究内容主要包括以下三个方面：

第一，静态镜头参数生成方法研究。静态镜头是虚拟摄像机控制的基础问题，其核心在于根据用户的镜头描述确定摄像机的最佳观察位置与姿态。本文首先设计镜头语义本体（Cinematographic Ontology），建立涵盖景别、角度、构图等维度的形式化描述框架，为自然语言与摄像机参数之间搭建语义桥梁。在此基础上，设计面向镜头理解任务的提示模板，利用大语言模型将用户的自然语言输入解析为结构化的镜头意图表示。随后，将镜头意图中的各项语义要素转化为摄像机配置的几何约束，包括基于景别的距离约束、基于角度描述的方位约束、基于构图规则的屏幕位置约束等。最后，设计参数求解算法，在约束空间中计算满足全部条件的摄像机参数。

第二，动态运镜轨迹生成方法研究。动态运镜是静态镜头在时间维度上的自然延伸，涉及摄像机位置与姿态随时间变化的轨迹规划问题。本文首先对常见的基础运镜类型（推、拉、摇、移、升降、环绕等）进行形式化建模，明确各类运镜的几何特征与参数空间。在大语言模型解析层面，扩展静态镜头的提示模板以支持运镜意图的识别与参数提取，包括运镜类型、运动方向、速度特征、起止状态等。在轨迹生成层面，采用参数化曲线（如贝塞尔曲线）表示摄像机的空间路径，并结合球面线性插值（Slerp）处理旋转姿态的平滑过渡，确保生成的运镜轨迹在视觉上流畅自然。

第三，系统设计与引擎集成。在完成核心算法研究的基础上，本文设计并实现一个完整的原型系统，将上述方法集成到Unreal Engine 5平台中。系统采用模块化架构，主要包括：自然语言处理模块，负责调用大语言模型接口完成语义解析；参数计算模块，实现从结构化意图到摄像机参数的几何求解；引擎交互模块，负责与UE5的通信以及摄像机状态的实时控制。

\subsection{主要贡献}

本文的主要贡献概括如下：

第一，提出镜头语义本体与混合控制框架。本文设计了一套面向虚拟摄像机控制的镜头语义本体，将电影摄影学中的景别、角度、运动等概念形式化为可计算的语义结构。在此基础上，提出“大语言模型语义解析—几何约束建模—参数优化求解”的混合控制框架，既发挥了大语言模型在自然语言理解方面的优势，又保留了传统方法在几何计算方面的精确性，实现了两类技术的有效互补。

第二，设计面向静态镜头与动态运镜的参数生成算法。针对静态镜头问题，本文提出基于多约束融合的摄像机参数求解方法，将景别、角度、构图等语义要素统一转化为几何约束并进行联合优化。针对动态运镜问题，本文提出基于参数化曲线的轨迹生成方法，结合运镜类型的形式化模型实现从语义描述到平滑轨迹的自动转换。

第三，实现面向UE5的原型系统并进行实验验证。本文在Unreal Engine 5.3.2平台上实现了完整的原型系统，支持自然语言输入、实时参数生成与可视化预览。通过构图质量评估、语义一致性测试和用户研究等多维度实验，验证了所提方法的有效性与实用性，为自然语言驱动的虚拟摄像机控制提供了可行的技术方案。

\section{论文组织结构}

本文共分为七章，各章内容安排如下：

第一章为绪论。本章首先阐述虚拟制作技术发展背景下虚拟摄像机智能控制的研究动机，分析当前参数化交互方式存在的认知鸿沟问题；随后从虚拟摄像机自动控制、大语言模型与结构化输出、自然语言驱动的三维场景交互三个维度综述国内外研究现状，指出现有工作的不足；最后明确本文的研究目标、研究内容与主要贡献。

第二章为相关工作。本章对论文涉及的基础理论与关键技术进行详细介绍，包括：虚拟摄像机控制的经典方法（约束满足、优化方法、Toric Space等）、大语言模型的技术原理与提示工程方法、电影摄影学中的镜头语言体系（景别、角度、运镜类型），为后续章节的方法设计奠定理论基础。

第三章为静态镜头参数生成方法。本章聚焦于核心工作一，研究如何从自然语言描述生成单帧静态镜头的摄像机参数。首先给出问题的形式化定义，明确输入输出规范；然后详细阐述镜头语义本体的设计、大语言模型提示模板的构建、以及结构化输出的解析方法；接着将语义要素转化为几何约束，设计景别约束、角度约束、构图约束的数学建模方法；最后提出参数求解算法，实现从约束集合到最优摄像机配置的计算。

第四章为动态运镜轨迹生成方法。本章聚焦于核心工作二，研究如何从自然语言描述生成摄像机的运动轨迹。首先将动态运镜问题定义为静态问题在时间维度上的扩展；然后对基础运镜类型（推、拉、摇、移、升降、环绕等）进行形式化建模，建立运镜语义与轨迹参数的映射关系；接着扩展大语言模型的提示设计以支持运镜意图解析；最后采用贝塞尔曲线进行轨迹参数化表示，结合球面线性插值实现旋转姿态的平滑过渡。

第五章为系统设计与实现。本章介绍原型系统的整体架构与实现细节。首先概述系统的模块划分与数据流设计；然后分别阐述自然语言处理模块、参数计算模块的实现方案；接着详细介绍与Unreal Engine 5的集成方法，包括通信机制、摄像机控制接口、场景信息获取等；最后说明迭代优化交互机制的设计与实现。

第六章为实验与评估。本章对所提方法与系统进行全面的实验验证。首先介绍实验环境与测试场景的设置；然后从构图质量、语义一致性等维度对静态镜头生成结果进行定量评估；接着对动态运镜的轨迹平滑度与语义匹配度进行评估；随后设计用户研究实验，通过对比实验和问卷调查评估系统的可用性与用户满意度；最后通过典型案例分析展示系统的实际应用效果，并讨论方法的局限性。

第七章为总结与展望。本章对全文工作进行总结，归纳本文的主要研究成果与贡献，并对未来可能的研究方向进行展望，包括遮挡感知、多目标协同、复合运镜支持等扩展问题。

% =============================================================================
% 第二章 相关理论与技术基础
% =============================================================================
\chapter{相关理论与技术基础}
\label{chap:theory}

\section{虚拟摄像机控制方法}

虚拟摄像机控制是计算机图形学中的经典问题，其核心任务是根据给定的拍摄需求自动确定摄像机的配置参数\cite{christie2008camera}。本节首先介绍虚拟摄像机的参数表示方法，随后回顾基于约束的控制框架，最后简述Toric Space理论，为本文的方法设计提供理论基础。

\subsection{摄像机参数表示}

在三维虚拟环境中，摄像机的状态通常由外参（Extrinsic Parameters）和内参（Intrinsic Parameters）共同描述\cite{hartley2003multiple}。本文关注的虚拟摄像机控制问题主要涉及外参与部分内参的确定。

摄像机的外参描述其在世界坐标系中的位姿，包括位置与旋转两部分。位置通常采用三维笛卡尔坐标$\mathbf{p} = (x, y, z)^T$表示，定义摄像机光心在世界坐标系中的空间坐标。旋转则描述摄像机坐标系相对于世界坐标系的姿态变换，常用的表示方法有欧拉角和四元数两种。欧拉角以三个角度值$(\phi, \theta, \psi)$分别表示绕三个坐标轴的旋转量，直观易懂但存在万向节死锁（Gimbal Lock）问题。四元数采用四维向量$\mathbf{q} = (q_w, q_x, q_y, q_z)$表示旋转，可避免万向节死锁，且便于进行旋转插值\cite{shoemake1985animating,hanson2005visualizing}。在Unreal Engine中，摄像机的旋转内部采用四元数存储，但对外提供欧拉角形式的接口（Pitch、Yaw、Roll），本文在参数计算时综合使用两种表示方式。

在内参方面，本文主要关注视场角（Field of View, FOV）这一参数。视场角定义了摄像机的可视范围，通常以垂直方向的张角度数表示。视场角与焦距$f$、传感器尺寸$h$之间存在如下关系：
\begin{equation}
\text{FOV} = 2 \arctan\left(\frac{h}{2f}\right)
\end{equation}
较小的视场角产生类似长焦镜头的压缩效果，较大的视场角则产生广角镜头的扩张效果。视场角的选择直接影响画面的空间感与景别呈现，是本文参数生成需要考虑的重要因素。

综上，本文将虚拟摄像机的完整状态表示为：
\begin{equation}
\mathbf{C} = (\mathbf{p}, \mathbf{R}, \text{FOV}) = (x, y, z, \phi, \theta, \psi, \text{FOV})
\end{equation}
其中$\mathbf{p}$为位置向量，$\mathbf{R}$为旋转（以欧拉角或四元数表示），FOV为视场角。给定用户的镜头描述与场景信息，本文的任务即是求解满足要求的摄像机状态$\mathbf{C}$。

\subsection{基于约束的控制框架}

基于约束的摄像机控制方法将镜头配置问题建模为约束满足问题（Constraint Satisfaction Problem, CSP），通过定义一组视觉约束条件，搜索满足全部约束的摄像机参数\cite{bares1999constraint}。这一框架为本文的参数求解方法提供了重要参考。

在约束满足框架中，常见的约束类型包括：

（1）可见性约束。要求目标物体在摄像机视野范围内可见，即目标的投影落在画面边界之内。设目标物体在世界坐标系中的位置为$\mathbf{t}$，经摄像机投影后的屏幕坐标为$(u, v)$，可见性约束可表示为：
\begin{equation}
0 \leq u \leq W, \quad 0 \leq v \leq H
\end{equation}
其中$W$、$H$为画面的宽度和高度。

（2）屏幕位置约束。要求目标物体在画面中呈现于特定区域，用于实现构图控制。例如，三分法构图要求主体位于画面三分线附近，可表示为目标投影坐标与期望位置之间的距离约束。

（3）距离约束。控制摄像机与目标之间的空间距离，用于实现不同景别的呈现。设摄像机位置为$\mathbf{p}$，目标位置为$\mathbf{t}$，距离约束可表示为：
\begin{equation}
d_{\min} \leq \|\mathbf{p} - \mathbf{t}\| \leq d_{\max}
\end{equation}
其中$d_{\min}$、$d_{\max}$为景别对应的距离范围。

（4）角度约束。控制摄像机相对于目标的观察方向，用于实现俯拍、仰拍等角度效果。可通过约束摄像机位置相对于目标的方位角和俯仰角来实现。

在求解方法上，早期工作多采用启发式搜索算法在参数空间中寻找可行解\cite{gleicher1992through}。后续研究将约束条件转化为优化目标函数，采用梯度下降、进化算法等优化方法求解最优配置\cite{lino2015intuitive,jiang2020optimizing}。本文借鉴这一框架的核心思想，将大语言模型解析得到的镜头语义转化为几何约束，再通过参数计算求解满足约束的摄像机状态。

\subsection{Toric Space理论}

Toric Space（环面空间）是Lino和Christie\cite{lino2015intuitive}提出的一种面向双目标构图问题的摄像机参数化方法，为复杂构图需求下的摄像机控制提供了优雅的数学框架。

在双目标拍摄场景中，用户通常希望两个目标同时出现在画面中，并对各自的屏幕位置有特定要求。传统方法需要在高维参数空间中搜索满足条件的摄像机配置，计算复杂度较高。Toric Space的核心思想是将摄像机位置参数化到一个以两目标连线为轴的环面流形上，从而将问题的自由度从六维降至三维（环面上的两个角度参数加上距离参数），显著简化了求解过程。

具体而言，设两目标位置分别为$\mathbf{A}$和$\mathbf{B}$，Toric Space将摄像机位置表示为：
\begin{equation}
\mathbf{p}(\alpha, \theta, r) = \mathbf{M} + r \cdot \mathbf{d}(\alpha, \theta)
\end{equation}
其中$\mathbf{M}$为两目标连线的中点，$\alpha$为绕连线轴的旋转角，$\theta$为与连线的夹角，$r$为到中点的距离，$\mathbf{d}(\alpha, \theta)$为由角度参数确定的方向向量。通过这种参数化，在环面空间中的连续移动可自然地保持两目标在画面中的相对位置关系，便于实现平滑的构图调整与运镜过渡。

虽然本文的研究场景以单目标为主，但Toric Space提供的参数化思想——通过几何变换将摄像机配置映射到低维流形上——对本文的环绕运镜建模具有重要启发意义。

\section{大语言模型与结构化输出}

大语言模型是本文实现自然语言理解与镜头意图解析的核心技术基础。本节介绍大语言模型的指令遵循能力、结构化输出机制以及提示工程的关键技术，为本文LLM模块的设计提供方法依据。

\subsection{大语言模型的指令遵循能力}

大语言模型（Large Language Model, LLM）是基于Transformer架构\cite{vaswani2017attention}、在大规模文本语料上预训练得到的神经网络模型。近年来，以GPT-4\cite{openai2023gpt4}、LLaMA\cite{touvron2023llama,touvron2023llama2}为代表的大语言模型在参数规模和能力边界上持续突破，展现出强大的自然语言理解与生成能力。

对于本文的应用场景而言，大语言模型的指令遵循（Instruction Following）能力是最为关键的特性。早期的语言模型主要针对文本续写任务进行优化，难以准确理解和执行用户的具体指令。Ouyang等人\cite{ouyang2022training}提出的指令微调（Instruction Tuning）方法通过在人工标注的指令-响应数据对上进行监督微调，并结合基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF），显著提升了模型对用户意图的理解能力和响应质量。经过指令微调的模型能够准确识别用户请求的任务类型，并按照指定的格式和要求生成输出，这为本文利用大语言模型解析镜头描述、生成结构化意图表示提供了技术可行性。

此外，大语言模型在训练过程中从海量文本中习得了丰富的世界知识，包括电影摄影、空间关系等领域的常识。这意味着模型对“近景”“俯拍”“环绕”等镜头术语具有基本的语义理解能力，能够将这些概念与具体的视觉效果相关联，从而在无需额外领域训练的情况下完成镜头描述的语义解析任务。

\subsection{结构化输出与JSON Schema}

本文的技术路线要求大语言模型将自然语言形式的镜头描述转换为可供后续算法处理的结构化数据。结构化输出（Structured Output）是指模型按照预定义的格式（如JSON、XML）生成具有固定字段和数据类型的输出内容\cite{geng2025jsonschemabench}。

JSON（JavaScript Object Notation）因其简洁的语法和良好的可读性，成为大语言模型结构化输出的主流格式。通过在提示中明确指定输出的JSON结构，模型能够生成符合要求的键值对数据。例如，对于镜头描述“给火箭一个仰拍的近景”，期望模型输出如下结构：

\begin{verbatim}
{
  "shot_type": "close_up",
  "vertical_angle": "low_angle",
  "target": "rocket"
}
\end{verbatim}

为进一步约束输出格式的规范性，可采用JSON Schema定义输出的结构规范，包括必需字段、字段类型、枚举值范围等。现代大语言模型的API（如OpenAI的GPT系列）已原生支持JSON模式，可在一定程度上保证输出的格式正确性。然而，模型仍可能产生不符合预期的字段值或遗漏必要字段，因此本文在系统实现中设计了输出校验与容错机制，确保解析结果的可靠性。

\subsection{提示工程方法}

提示工程（Prompt Engineering）是指通过设计和优化输入提示来引导大语言模型生成期望输出的技术方法\cite{sahoo2024systematic}。合理的提示设计对于充分发挥模型能力、获得高质量输出至关重要。

本文的提示设计主要借鉴以下几类技术：

（1）系统提示与角色设定。通过系统提示（System Prompt）为模型设定专业角色（如“你是一位专业的电影摄影师”），引导模型从领域专家的视角理解和处理用户输入，提升输出的专业性和准确性。

（2）少样本提示（Few-shot Prompting）。在提示中提供若干输入-输出示例，帮助模型理解任务要求和输出格式。对于镜头解析任务，示例可展示不同类型镜头描述与对应结构化输出的映射关系，使模型通过类比学习掌握转换规则。

（3）思维链提示（Chain-of-Thought Prompting）。Wei等人\cite{wei2022chain}发现，引导模型逐步推理而非直接给出答案，可显著提升其在复杂任务上的表现。对于本文的镜头解析任务，可引导模型先识别描述中的关键要素（目标物体、景别、角度等），再逐一映射为结构化字段，提高解析的准确性和完整性。

（4）输出格式约束。在提示中明确指定输出必须为JSON格式，并给出字段定义和取值说明。通过在提示末尾添加输出格式模板或起始标记（如“请以JSON格式输出：”），可有效引导模型生成符合要求的结构化内容。

本文综合运用上述技术设计镜头解析的提示模板，具体的提示结构与示例将在第三章详细阐述。

\section{电影摄影学中的镜头语言}

镜头语言是电影摄影学的核心概念体系，为影视创作者提供了描述和交流视觉表达意图的专业词汇\cite{arijon1976grammar,mascelli1965five}。本文构建的镜头语义本体直接基于这一概念体系，因此有必要对其中与虚拟摄像机参数密切相关的三个维度——景别、拍摄角度、运镜类型——进行系统梳理。

\subsection{景别分类}

景别（Shot Scale）是描述画面中主体呈现范围的术语，反映摄像机与拍摄对象之间的相对距离关系\cite{rao2020unified}。不同景别传递不同的视觉信息量和情感基调，是镜头设计的基础要素。根据主体在画面中的占比，景别通常分为以下几类：

（1）大远景（Extreme Long Shot）。画面以广阔的环境为主，人物或主体在画面中占比极小，主要用于展示宏大场景、建立空间背景。

（2）远景（Long Shot）。主体完整呈现于画面中，周围环境占据较大比例，用于交代主体与环境的关系。

（3）全景（Full Shot）。以人物为例，画面恰好容纳人物全身，主体与环境各占一定比例，兼顾人物动作与场景信息。

（4）中景（Medium Shot）。画面呈现人物膝盖或腰部以上的范围，是叙事性场景中最常用的景别，平衡了人物表现与环境交代。

（5）近景（Medium Close-up）。画面呈现人物胸部以上的范围，突出人物表情和上半身动作，适合对话场景。

（6）特写（Close-up）。画面聚焦于人物面部或物体细节，强调情感表达或关键信息，具有强烈的视觉冲击力。

（7）大特写（Extreme Close-up）。画面呈现人物面部局部（如眼睛）或物体的极小细节，用于营造强烈的戏剧张力。

从几何角度分析，景别主要由摄像机与主体之间的距离$d$以及视场角FOV共同决定。设主体的特征尺寸为$h$（如人物身高），主体在画面中的占比为$\rho$，则三者之间的近似关系为：
\begin{equation}
\rho \approx \frac{h}{2d \cdot \tan(\text{FOV}/2)}
\end{equation}
本文在第三章将基于这一关系，将景别语义转化为距离约束。

\subsection{拍摄角度}

拍摄角度描述摄像机相对于主体的空间方位，包括垂直角度和水平角度两个维度，对画面的视觉效果和心理暗示具有重要影响。

垂直角度指摄像机光轴与水平面之间的夹角，主要包括：

（1）俯拍（High Angle）。摄像机位于主体上方向下拍摄，使主体显得渺小、弱势，常用于表现压迫感或全局视角。

（2）平拍（Eye Level）。摄像机与主体大致平齐，呈现客观、中立的视角，是最常用的拍摄角度。

（3）仰拍（Low Angle）。摄像机位于主体下方向上拍摄，使主体显得高大、威严，常用于塑造英雄形象或表现崇敬感。

水平角度指摄像机在水平面上相对于主体朝向的方位，主要包括：

（1）正面（Frontal）。摄像机正对主体，直接呈现主体正面特征。

（2）侧面（Profile）。摄像机位于主体侧方，呈现主体轮廓，常用于对话场景的正反打。

（3）斜侧面（Three-quarter）。介于正面与侧面之间的角度，兼具正面信息和空间层次感，是人像拍摄的常用角度。

（4）背面（Rear）。摄像机位于主体背后，常用于建立主体视角或营造神秘感。

从几何角度分析，设主体位置为$\mathbf{t}$、朝向为$\mathbf{f}$，摄像机位置为$\mathbf{p}$，则水平角度$\alpha$和垂直角度$\beta$可表示为：
\begin{equation}
\alpha = \arccos\left(\frac{(\mathbf{p} - \mathbf{t})_{\text{horizontal}} \cdot \mathbf{f}_{\text{horizontal}}}{\|(\mathbf{p} - \mathbf{t})_{\text{horizontal}}\| \cdot \|\mathbf{f}_{\text{horizontal}}\|}\right)
\end{equation}
\begin{equation}
\beta = \arctan\left(\frac{p_z - t_z}{\|(\mathbf{p} - \mathbf{t})_{\text{horizontal}}\|}\right)
\end{equation}
其中下标horizontal表示向量在水平面上的投影。本文在参数计算中将依据上述关系实现角度约束的建模。

\subsection{运镜类型}

运镜（Camera Movement）指摄像机在拍摄过程中的运动方式，通过位置或姿态的变化创造动态的视觉效果\cite{arijon1976grammar}。根据摄像机的运动特征，基础运镜类型可分为以下几类：

固定机位运镜指摄像机位置不变，仅改变拍摄方向或焦距：

（1）摇（Pan）。摄像机绕垂直轴水平旋转，画面在水平方向上扫视，用于展示宽广场景或跟随水平运动的主体。

（2）俯仰（Tilt）。摄像机绕水平轴垂直旋转，画面在垂直方向上扫视，用于展示高耸物体或跟随垂直运动的主体。

（3）变焦（Zoom）。通过改变镜头焦距调整视场角，在不移动摄像机的情况下实现画面的推近或拉远效果。

移动机位运镜指摄像机位置发生变化：

（1）推（Dolly In）。摄像机沿光轴方向向主体移动，画面逐渐逼近主体，用于强调、聚焦或进入场景。

（2）拉（Dolly Out）。摄像机沿光轴方向远离主体，画面逐渐展开，用于揭示环境、结束场景或制造疏离感。

（3）横移（Truck）。摄像机沿垂直于光轴的水平方向移动，画面平行滑动，用于跟随主体水平运动或展示场景横向延展。

（4）升降（Pedestal）。摄像机沿垂直方向升高或降低，用于改变视角高度或跟随主体垂直运动。

（5）环绕（Orbit）。摄像机绕主体做圆周运动，同时保持朝向主体，用于全方位展示主体或营造戏剧性效果。

从运动学角度分析，各类运镜可抽象为摄像机位置$\mathbf{p}(t)$和旋转$\mathbf{R}(t)$随时间变化的轨迹函数。本文在第四章将对上述运镜类型进行形式化建模，建立运镜语义与轨迹参数之间的映射关系。

\section{本章小结}

本章从三个维度介绍了本文研究所依赖的技术基础与领域知识。

在虚拟摄像机控制方法方面，本章首先明确了摄像机状态的参数表示方式，包括位置、旋转和视场角，为后续的参数计算提供了形式化基础；随后介绍了基于约束的控制框架，阐述了可见性约束、屏幕位置约束、距离约束和角度约束的数学建模方法，本文的参数求解算法将借鉴这一框架的核心思想；最后简述了Toric Space理论的参数化思想，为环绕运镜的轨迹建模提供了参考。

在大语言模型方面，本章重点介绍了指令遵循能力、结构化输出机制和提示工程方法。指令微调技术使大语言模型能够准确理解用户意图并按指定格式生成输出，为本文利用大语言模型解析镜头描述提供了技术基础；JSON Schema约束和提示工程技术则为设计可靠的语义解析模块提供了方法指导。

在电影摄影学方面，本章系统梳理了景别、拍摄角度和运镜类型三类核心概念，并分析了各概念与摄像机几何参数之间的对应关系。这些镜头语言概念将直接构成本文镜头语义本体的核心维度，其几何特征分析则为第三章的约束建模和第四章的轨迹生成奠定了理论基础。

综上，本章所述内容为后续章节的方法设计提供了必要的理论准备：第三章将结合约束控制框架与大语言模型的结构化输出能力，实现静态镜头的参数生成；第四章将基于运镜类型的形式化定义，实现动态运镜的轨迹生成。

% =============================================================================
% 第三章 核心工作一：静态镜头参数生成方法
% =============================================================================

% =============================================================================
% 第四章 核心工作二：动态运镜轨迹生成方法
% =============================================================================

% =============================================================================
% 第五章 系统实现与实验分析
% =============================================================================

% =============================================================================
% 第六章 实验分析与系统评估
% =============================================================================

% =============================================================================
% 第七章 总结与展望
% =============================================================================
