% =============================================================================
% 第一章 绪论 (预计 8 页)
% =============================================================================
\chapter{绪论}
\label{chap:intro}

\section{研究背景与意义}

近年来，随着元宇宙、数字孪生以及 3A 游戏（如 Unreal Engine 5 驱动的影视级游戏）的兴起，各行各业对高质量三维（3D）数字内容的需求呈现爆发式增长。传统的生产流程往往成本高昂且周期漫长，而\textbf{生成式人工智能（Artificial Intelligence Generated Content, AIGC）}技术的突破为这一困境带来了新的解决思路。特别是随着文生图（Text-to-Image）和文生三维（Text-to-3D）算法的成熟，创作者希望能够通过简单的自然语言指令，直接指挥计算机生成复杂的场景与镜头。这使得“智能化内容生产”成为当前计算机图形学与人工智能交叉领域的研究热点。

然而，在当前的三维内容生产管线中，\textbf{“虚拟运镜控制”}（Virtual Camera Control）依然是一个极具挑战性的技术瓶颈。这本质上是一个典型的\textbf{“语义鸿沟”}（Semantic Gap）问题：艺术家和导演习惯使用抽象的感性语言（例如“具有压迫感的仰视”、“黄金分割构图”）来描述镜头意图；而三维渲染引擎（如 Unity, Unreal Engine）的摄像机系统则依赖于底层的、精确的工程参数（如世界坐标位置、欧拉角旋转、视场角 FOV 等）。这种“感性视觉思维”与“理性数值逻辑”的不匹配，导致创作者往往需要陷入漫长的“试错循环”，通过手动反复调整参数来逼近预期的构图效果，严重制约了创作效率。

针对这一问题，现有的解决方案主要分为两类，但均存在显著局限性：
第一类是\textbf{基于规则的辅助工具}。这类方法通常预设了一些标准的运镜模板（如环绕、推拉），虽然保证了参数的精确性，但缺乏语义理解能力，无法灵活响应复杂的、个性化的自然语言指令。
第二类是新兴的\textbf{端到端 AIGC 技术}。例如 DreamFusion 等文生三维模型，虽然能直接生成三维物体，但其生成的几何拓扑往往杂乱无章，难以被标准的工业化管线所兼容和二次编辑。此外，现有的文生图模型虽然能生成极具艺术感的二维图像，但缺乏空间深度信息，无法直接驱动三维场景中的摄像机。

鉴于此，本文提出了一种\textbf{“基于 AIGC 分镜头引导的虚拟摄像机参数逆向求解方法”}。本研究的核心思路是\textbf{“以图生参”}：创新性地利用成熟的文生图大模型作为\textbf{“中间语义层”}，先将抽象的自然语言转化为直观的二维参考图像；随后，通过本文提出的\textbf{几何逆向投影算法}，从二维图像中提取构图特征，精确解算出三维场景中的摄像机位姿参数。该方法不仅有效消除了语义鸿沟，实现了“所见即所得”的自动化运镜，还具备针对不同尺度物体（从微观昆虫到宏观航天器）的\textbf{尺度自适应能力}，具有重要的理论意义与工程应用价值。

\section{国内外研究现状}

\subsection{虚拟运镜控制技术研究现状}

虚拟摄像机控制（Virtual Camera Control）作为连接三维场景与二维画面的桥梁，一直是计算机图形学领域的核心议题。经过三十余年的发展，相关研究已从早期的手动关键帧插值，演变为如今涵盖几何约束求解、路径规划优化、计算美学评价及智能学习决策的多元化技术体系。

\subsubsection{基于约束满足与规则的相机控制}
早期的自动化运镜研究主要致力于将电影摄影中的美学法则转化为数学上的几何约束。Gleicher 等人 \cite{gleicher1992through} 最早提出了“透过镜头”（Through-the-Lens）的控制范式，允许用户直接在屏幕空间操纵物体的位置来反推相机参数。随后，Bares 等人 \cite{bares1999constraint} 开发了基于约束的相机规划器，能够处理包括构图、遮挡和视角在内的多重约束冲突。
Christie 等人 \cite{christie2008camera} 对此类方法进行了系统性总结，并提出了基于“镜头习语”（Cinematographic Idioms）的语义框架 \cite{olivier2009interactive}。该框架通过预定义如“过肩镜头”、“三分法构图”等模板，将运镜问题转化为约束满足问题（CSP）。
为了量化构图质量，Liu 等人 \cite{liu2010optimizing} 和 Dunkel 等人 \cite{dunkel2017image} 引入了基于显著性图（Saliency Map）和视觉平衡（Visual Balance）的计算美学指标，为自动化构图提供了数学评价标准。
尽管此类方法保证了构图的精确性，但其高度依赖专家构建的规则库，面对复杂动态场景时往往缺乏灵活性。

\subsubsection{基于搜索与优化的路径规划}
为了解决规则系统的僵化问题，基于优化的方法将运镜视为高维空间中的代价最小化问题。Li 等人 \cite{li2008real} 提出了一种实时相机规划算法，通过构建可见性图（Visibility Graph）来确保相机在漫游过程中始终避开障碍物。
Oskam 等人 \cite{oskam2009visibility} 针对动态场景设计了可见性转换规划器，利用随机采样策略在保证主体可见的同时实现平滑的视角切换。Lino 等人 \cite{lino2015intuitive} 则引入了“环面空间”（Toric Space）这一流形结构，将双主体构图问题从六维搜索空间降维至二维流形，极大地提升了求解效率。
Galvane 等人 \cite{galvane2015automated} 进一步提出了用于交互式叙事的自动化电影摄影系统，能够根据剧情紧张度动态调整相机节奏。

\subsubsection{基于智能学习的运镜决策}
近年来，深度强化学习（Deep Reinforcement Learning, DRL）为解决复杂的运镜决策提供了新思路。Chen 等人 \cite{chen2016learning} 训练了一个智能体来模仿专业摄影师的构图选择。Bonatti 等人 \cite{bonatti2020autonomous} 和 N{\"a}geli 等人 \cite{nageli2017real} 提出了一种用于无人机航拍的自主运镜系统，利用强化学习在实时飞行中平衡构图美学与避障安全。Huang 等人 \cite{huang2019throughput} 则探索了利用 DRL 进行“透过镜头”式的交互控制。
虽然学习型方法在风格迁移和动态适应方面表现优异，但其“黑盒”特性导致用户难以通过明确的语义指令（如“稍微往左一点”）来微调结果，可控性（Controllability）仍然是一个未解难题。

\subsection{AIGC驱动的三维内容生成研究现状}

生成式人工智能（AIGC）的爆发经历了从判别式模型到生成式模型的范式转移。从早期的生成对抗网络（GANs）到如今的扩散模型（Diffusion Models）与神经辐射场（NeRF），AI 在三维内容生产中的角色正从“辅助工具”转变为“核心创作者”。

\subsubsection{从GANs到扩散模型的二维图像生成}
在扩散模型统治该领域之前，生成对抗网络（GANs）\cite{goodfellow2014generative} 是图像生成的主流架构。Kingma 等人 \cite{kingma2013auto} 提出的变分自编码器（VAE）与 Karras 等人 \cite{karras2019style} 提出的 StyleGAN 系列，在人脸生成等特定领域达到了极高的逼真度。然而，GANs 普遍存在训练不稳定和模式坍塌（Mode Collapse）的问题。
2020年以来，Ho 等人 \cite{ho2020denoising} 提出的去噪扩散概率模型（DDPM）和 Song 等人 \cite{song2021denoising} 提出的 DDIM 奠定了现代生成模型的基础。随后，Rombach 等人 \cite{rombach2022high} 提出的潜在扩散模型（LDM/Stable Diffusion）结合 CLIP \cite{radford2021learning} 实现了文本驱动的高质量图像生成。
为了提升生成的可控性，Zhang 等人 \cite{zhang2023adding} 提出的 ControlNet，Mou 等人 \cite{mou2024t2i} 提出的 T2I-Adapter，以及 Li 等人 \cite{li2023gligen} 提出的 GLIGEN，使得用户可以通过边缘图、骨架、布局框等几何条件精确控制生成结果。Ye 等人 \cite{ye2023ip} 提出的 IP-Adapter 更是实现了基于参考图的风格与内容迁移。尽管如此，上述工作主要聚焦于二维像素平面的生成，缺乏三维空间的一致性。

\subsubsection{文本驱动的三维场景生成}
为了将 AIGC 拓展至三维空间，Mildenhall 等人 \cite{mildenhall2021nerf} 提出的神经辐射场（NeRF）为三维表示提供了新的微分可渲染形式。Google 的 DreamFusion \cite{poole2023dreamfusion} 开创性地提出了分数蒸馏采样（SDS），利用预训练的 2D 扩散模型指导 NeRF 的优化，实现了 Text-to-3D 的突破。
此后，Magic3D \cite{lin2023magic3d}、Fantasia3D \cite{chen2023fantasia3d} 以及 ProlificDreamer \cite{wang2023prolificdreamer} 等工作不断提升了生成的几何细节与纹理质量。
针对单图生成三维（Image-to-3D）任务，Liu 等人 \cite{liu2023zero} 提出的 Zero-1-to-3 利用视角条件扩散模型实现了零样本的新视角合成；OpenAI 的 Point-E \cite{nichol2022point} 和 Shap-E \cite{jun2023shap} 则探索了基于点云和隐式函数的快速生成路线。Shi 等人 \cite{shi2023mvdream} 提出的 MVDream 通过多视角一致性优化解决了几何畸变问题。
值得注意的是，Kerbl 等人 \cite{kerbl20233d} 于2023年提出的 3D Gaussian Splatting 技术，以其高保真和实时渲染特性，正在逐步取代 NeRF 成为新一代的三维表达标准。
然而，无论是基于 NeRF 还是 Gaussian Splatting 的生成方法，目前仍主要局限于\textbf{单个物体}的生成。对于包含复杂空间关系、需要精确运镜调度的\textbf{场景级}（Scene-level）生成任务，现有的端到端模型仍显得力不从心。这正是本文提出“基于分镜头引导”策略的切入点。

\section{本文主要研究内容}

针对三维内容生产中自然语言指令与底层工程参数之间的语义鸿沟问题，本文提出了一种基于 AIGC 分镜头引导的虚拟摄像机参数逆向求解方法。本文的主要研究内容如下：

\begin{enumerate}
    \item \textbf{基于 AIGC 的跨模态语义交互机制研究}
    
    针对传统运镜控制缺乏语义理解能力的问题，本文构建了一种“文本-图像-参数”的跨模态交互框架。利用大语言模型（LLM）对用户输入的自然语言指令进行意图解析与提示词优化，并驱动文生图（Text-to-Image）模型生成具备目标构图特征的二维参考图像。该机制有效地将抽象的视觉意图具象化为可视化的二维参考，为后续的参数求解提供了准确的语义锚点。

    \item \textbf{基于几何逆向投影的摄像机位姿解算算法研究}
    
    针对从二维参考图像恢复三维空间信息的难题，本文提出了一种基于视锥体几何约束的逆向投影算法。该算法建立了一个包含目标物体真实尺度、二维图像像素特征与虚拟摄像机参数的数学模型。通过提取参考图像中目标物体的轴对齐包围盒（AABB）特征，结合三角成像原理与空间变换矩阵，精确反解出虚拟摄像机在局部坐标系下的距离、偏移量及旋转欧拉角，实现了从“二维构图”到“三维位姿”的逆向映射。

    \item \textbf{场景尺度自适应与系统集成实现}
    
    为了解决单一算法难以适配多尺度目标物体（如从微观物体到宏观场景）的问题，本文设计了一种基于包围盒对角线的尺度归一化机制，实现了运镜距离的自适应动态调整。在此基础上，本文基于 Unreal Engine 5 引擎开发了原型系统，集成了 AIGC 推理后端与三维渲染前端，验证了所提方法在不同复杂场景下的有效性与鲁棒性。
\end{enumerate}

\section{本文组织结构}

本文共分为六章，各章节的具体安排如下：

\textbf{第一章 绪论}：阐述了虚拟运镜控制与 AIGC 内容生成的研究背景与意义，分析了当前技术面临的主要挑战与“语义鸿沟”问题。系统梳理了国内外在虚拟运镜控制、文本生成图像及文本生成三维内容等领域的研究现状，并总结了本文的主要研究内容与创新点。

\textbf{第二章 相关理论与技术基础}：介绍了本文涉及的核心理论与技术栈，包括针孔相机模型与三维几何变换基础、生成式人工智能模型（如 Stable Diffusion）的基本原理，以及 Unreal Engine 5 引擎的架构与开发接口，为后续章节的算法实现奠定理论基础。

\textbf{第三章 基于 AIGC 的跨模态语义解析与参考生成}：详细阐述了跨模态交互模块的设计与实现。重点介绍了如何利用大语言模型进行提示词工程优化，以及如何利用 ControlNet 等技术控制文生图模型生成具备特定构图特征的参考图像，并从图像中提取关键的几何特征。

\textbf{第四章 基于几何逆向投影的摄像机位姿求解}：本章是论文的核心部分，详细推导了从二维图像特征到三维摄像机参数的数学映射关系。包括拍摄距离的逆向解算、视锥体截面的空间偏移计算以及摄像机旋转姿态的确定方法，并提出了尺度自适应优化策略。

\textbf{第五章 系统实现与实验分析}：介绍了基于 UE5 的原型系统架构与关键模块实现。设计了多组对比实验，从构图还原度、尺度适应性及用户满意度等维度，对本文提出的方法进行了定性展示与定量评估，验证了方法的有效性。

\textbf{第六章 总结与展望}：总结全文的研究工作与主要结论，客观分析了当前方法存在的局限性，并对未来的研究方向（如支持动态视频生成、多机位协同调度等）进行了展望。

% =============================================================================
% 第二章 相关理论与技术基础 (预计 6 页)
% =============================================================================
\chapter{相关理论与技术基础}
\label{chap:background}

本章主要介绍支撑本文研究的核心理论与技术框架。首先阐述针孔摄像机模型及其坐标系变换原理，为后续的参数逆向求解奠定几何基础；其次介绍生成式人工智能中的潜在扩散模型（Latent Diffusion Models）及其可控生成机制；最后概述 Unreal Engine 5 引擎在虚拟制片中的应用及其底层架构。

\section{针孔摄像机模型与几何变换}

针孔摄像机模型（Pinhole Camera Model）是计算机图形学与计算机视觉中最基础的成像模型 \cite{hartley2003multiple, szeliski2022computer}。该模型描述了三维世界坐标系中的点如何通过透视投影映射到二维图像坐标系中。

\subsection{四大坐标系定义}
为了精确描述成像过程，通常需要定义以下四个坐标系及其转换关系：
\begin{itemize}
    \item \textbf{世界坐标系 (World Coordinate System, $O_w$)}：描述物体在三维空间中绝对位置的基准坐标系，通常记为 $(X_w, Y_w, Z_w)$。
    \item \textbf{摄像机坐标系 (Camera Coordinate System, $O_c$)}：以摄像机光心为原点，光轴方向为 $Z_c$ 轴建立的坐标系，记为 $(X_c, Y_c, Z_c)$。
    \item \textbf{图像物理坐标系 (Image Plane Coordinate System, $O_{xy}$)}：位于摄像机前方焦距 $f$ 处的成像平面，单位通常为毫米，记为 $(x, y)$。
    \item \textbf{像素坐标系 (Pixel Coordinate System, $O_{uv}$)}：以图像左上角为原点，描述像素行列号的离散坐标系，单位为像素 (pixel)，记为 $(u, v)$。
\end{itemize}

\subsection{成像投影方程}
三维空间中的一点 $P_w = [X_w, Y_w, Z_w]^T$ 映射到像素坐标 $p = [u, v]^T$ 的过程可以表示为一系列矩阵的乘积。

首先，从世界坐标系到摄像机坐标系的变换属于刚体变换（Rigid Body Transformation），由旋转矩阵 $R$ 和平移向量 $t$ 决定：
\begin{equation}
    \begin{bmatrix} X_c \\ Y_c \\ Z_c \end{bmatrix} = R \begin{bmatrix} X_w \\ Y_w \\ Z_w \end{bmatrix} + t
\end{equation}

其次，根据相似三角形原理，从摄像机坐标系投影到图像物理坐标系的关系为：
\begin{equation}
    x = f \frac{X_c}{Z_c}, \quad y = f \frac{Y_c}{Z_c}
\end{equation}

最后，结合像素平移 $(c_x, c_y)$ 和物理尺寸转换因子 $(d_x, d_y)$，可以将整个成像过程统一表示为齐次坐标下的矩阵形式：
\begin{equation}
    Z_c \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 
    \underbrace{
    \begin{bmatrix} 
        f_x & 0 & c_x \\ 
        0 & f_y & c_y \\ 
        0 & 0 & 1 
    \end{bmatrix}
    }_{\text{内参矩阵 } K}
    \underbrace{
    \begin{bmatrix} 
        R & t 
    \end{bmatrix}
    }_{\text{外参矩阵 } [R|t]}
    \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}
\end{equation}

其中，$K$ 为摄像机内参矩阵（Intrinsic Matrix），包含焦距 $f_x, f_y$ 和主点坐标 $c_x, c_y$；$[R|t]$ 为外参矩阵（Extrinsic Matrix），描述了摄像机在世界坐标系中的位姿。本文第四章提出的“逆向投影算法”，本质上就是已知 $u, v$ 和 $K$，在引入几何约束的情况下反求 $R$ 和 $t$ 的过程。

\section{生成式人工智能与扩散模型}

随着深度学习的发展，基于扩散概率模型（Diffusion Probabilistic Models）的生成式 AI 已成为图像生成领域的主流技术。

\subsection{潜在扩散模型 (Latent Diffusion Models)}
扩散模型的本质是学习一个可逆的去噪过程。Ho 等人 \cite{ho2020denoising} 提出的去噪扩散概率模型（DDPM）包含前向加噪和反向去噪两个阶段。由于直接在像素空间计算开销巨大，Rombach 等人 \cite{rombach2022high} 提出了潜在扩散模型（LDM），引入感知压缩技术将图像映射到低维潜在空间（Latent Space）。

LDM 的训练目标是最小化噪声预测误差：
\begin{equation}
    L_{LDM} = \mathbb{E}_{z, \epsilon, t} \left[ \| \epsilon - \epsilon_\theta(z_t, t, \tau_\theta(y)) \|_2^2 \right]
\end{equation}
其中，$\epsilon$ 为加入的高斯噪声，$\epsilon_\theta$ 为去噪网络（通常为 U-Net 架构），$z_t$ 为 $t$ 时刻的噪声潜变量，$\tau_\theta(y)$ 为条件编码器（如 CLIP Text Encoder \cite{radford2021learning}）。通过交叉注意力机制（Cross-Attention），$\tau_\theta(y)$ 将文本语义注入 U-Net 的每一层，从而实现文本对生成内容的语义控制。

\subsection{可控生成机制 (ControlNet)}
虽然 LDM 具备强大的生成能力，但仅靠文本难以精确控制生成图像的几何结构（如边缘、姿态、深度）。ControlNet \cite{zhang2023adding} 通过一种独特的神经网络架构设计解决了这一问题。它锁定预训练的大模型参数，同时创建一个可训练的“副本层”，并通过“零卷积”（Zero Convolution）层将两个分支连接起来。

在本文中，ControlNet 被用于增强文生图过程的结构约束。具体而言，系统利用 Canny 边缘检测算子或 Depth 深度估计算子提取参考图的几何特征，并将其作为额外条件输入 ControlNet。这确保了生成的二维参考图像不仅符合文本描述，还具备符合透视原理的几何结构，从而显著提高了后续参数逆向解算的鲁棒性。

\section{三维渲染引擎与虚拟制片}

Unreal Engine 5 (UE5) 是 Epic Games 开发的实时 3D 创作工具，其在虚拟制片（Virtual Production）领域有着广泛应用 \cite{gregory2018game}。

\subsection{UE5 坐标系统}
在进行算法移植时，坐标系的不一致是主要的工程挑战。计算机视觉领域通常使用右手坐标系（Right-handed），而 UE5 采用左手坐标系（Left-handed）：
\begin{itemize}
    \item \textbf{X轴}：前后方向（Forward），正方向向前。
    \item \textbf{Y轴}：左右方向（Right），正方向向右。
    \item \textbf{Z轴}：上下方向（Up），正方向向上。
\end{itemize}

因此，从算法模块输出的位姿矩阵 $T_{algo}$ 转换到 UE5 引擎使用的位姿 $T_{ue}$，需要经过如下的基底变换：
\begin{equation}
    T_{ue} = M_{coord} \cdot T_{algo} \cdot M_{coord}^{-1}
\end{equation}
其中 $M_{coord}$ 为坐标轴翻转矩阵（通常涉及 Y 轴的取反）。

\subsection{CineCameraActor 组件}
UE5 内置的 \texttt{CineCameraActor} 是模拟真实电影摄影机的核心组件。它不仅包含位置和旋转属性，还完整模拟了物理相机的光学特性，如焦距（Focal Length）、光圈（Current Aperture）、传感器尺寸（Filmback）等。
本文提出的系统通过 Python 接口（Unreal Python API）与 \texttt{CineCameraActor} 进行交互，将解算出的参数实时映射到虚拟摄像机的属性上，从而驱动视口中的画面更新。

\section{本章小结}
本章系统介绍了针孔摄像机成像几何、潜在扩散模型原理以及 UE5 引擎架构。成像几何模型为从二维图像反推三维参数提供了理论依据；扩散模型与 ControlNet 为跨模态参考图像的生成提供了技术支撑；UE5 引擎则构成了算法验证的工程载体。这些理论与技术共同构成了本文研究的坚实基础。

% =============================================================================
% 第三章 基于视觉先验的静态构图参数逆向解算 (核心工作一，预计 18 页)
% =============================================================================
\chapter{基于视觉先验的静态构图参数逆向解算}
\label{chap:method_static}

\section{引言}
% TODO: 阐述本章解决的核心问题：如何把一张 2D 图片精确还原为 3D 摄像机参数。

\section{跨模态语义交互框架}
\subsection{基于LLM的视觉提示词优化}
% TODO: 详细描述如何将用户输入的“火箭飞行”转化为 SD 能理解的 "cinematic shot, rocket flying..."。
\subsection{生成式视觉先验获取}
% TODO: 描述 Stable Diffusion 的调用流程及参数设置。

\section{二维构图特征提取与分析}
% TODO: 描述使用 YOLO 提取包围盒 $(cx, cy, w, h)$ 的过程。
% TODO: 增加：数据预处理与异常值过滤。

\section{几何约束下的参数逆向求解算法}
\subsection{尺度归一化与局部坐标系构建}
% TODO: 对应专利步骤 S1，解释 AABB 包围盒及对角线 $L_{diag}$ 的计算。
\subsection{基于视锥体几何的初值估计}
% TODO: 对应专利步骤 S3，推导距离 $D$ 和 偏移量 $\Delta$ 的解析公式。
\begin{equation}
    D = \frac{H_{obj}}{2 \tan(\theta_{obj}/2)}
\end{equation}

\section{基于重投影误差的位姿迭代优化}
% TODO: 【这是为了增加学术深度的扩展部分】
\subsection{重投影损失函数定义}
% TODO: 定义 Loss = IoU_Loss + Center_Distance_Loss。
\subsection{非线性优化求解策略}
% TODO: 描述如何通过梯度下降或简单的迭代反馈来微调摄像机参数，解决透视畸变问题。

\section{本章小结}

% =============================================================================
% 第四章 尺度自适应与动态运镜生成 (核心工作二，预计 14 页)
% =============================================================================
\chapter{尺度自适应与动态运镜生成}
\label{chap:method_dynamic}

\section{引言}
% TODO: 阐述问题：静态构图只是起点，动态运镜和多尺度适配才是工业痛点。

\section{尺度自适应机制}
\subsection{场景尺度度量标准}
% TODO: 深入讨论为什么用包围盒对角线作为归一化因子是鲁棒的。
\subsection{运镜模板的参数化映射}
% TODO: 解释如何将标准模板库（Template Library）映射到不同大小的物体上。

\section{语义驱动的轨迹重定向}
\subsection{运镜语义特征匹配}
% TODO: 如何通过 LLM 分析指令中的动词（如“环绕”、“推进”），并检索对应的模板。
\subsection{关键帧插值与平滑处理}
% TODO: 描述在 UE5 中生成 Level Sequence 的过程，以及如何保证曲线平滑。

\section{实验与结果分析}
% TODO: 这里可以放针对动态效果的对比实验。
\subsection{多尺度适配性测试}
\subsection{动态轨迹平滑度分析}

\section{本章小结}

% =============================================================================
% 第五章 系统设计与实现 (预计 12 页)
% =============================================================================
\chapter{系统设计与实现}
\label{chap:system}

\section{需求分析}
% TODO: 功能需求（语义理解、参数解算、预览复现）与非功能需求（响应速度、易用性）。

\section{系统架构设计}
\subsection{总体架构}
% TODO: 绘制系统架构图（前端 UE5 Widget <-> 后端 Python Server <-> AI Models）。
\subsection{模块划分}
% TODO: 详细介绍 场景感知模块、跨模态交互模块、参数解算模块。

\section{关键功能模块实现}
\subsection{UE5 与外部 AI 服务的通信接口}
% TODO: 贴关键代码（Socket/HTTP 通信）。
\subsection{自动化管线集成}
% TODO: 描述 TAPython 脚本如何控制 CineCameraActor。

\section{系统测试与评估}
\subsection{功能测试}
\subsection{性能评估}
% TODO: 分析系统生成一次镜头的耗时（分解为 AI 生成时间 + 解算时间）。

\section{本章小结}

% =============================================================================
% 第六章 总结与展望 (预计 4 页)
% =============================================================================
\chapter{总结与展望}
\label{chap:concl}

\section{全文总结}
% TODO: 概括本文解决的问题、提出的方法以及取得的成果。

\section{研究局限}
% TODO: 诚实地列出不足（例如：目前仅支持单主体、极度夸张的透视生成可能失败）。

\section{未来工作展望}
% TODO: 多物体构图、光照与风格迁移的同步控制等。